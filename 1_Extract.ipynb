{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "import io\n",
        "import pandas as pd\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pickle\n",
        "from datetime import datetime\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "from datetime import timezone\n",
        "import json\n",
        "import time"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 1. Obtaining Course Eval URLS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import io\n",
        "import pandas as pd\n",
        "sub = pd.read_csv(io.StringIO(SUBJECT_COOKIES), header=None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "sub = sub.set_index(0)[1].to_dict()\n",
        "server = requests.Session()\n",
        "server.cookies.update(sub)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_pages_by_subject(server, urls, path = 'data/raw/subject_pages.jsonl'):\n",
        "\n",
        "    f = open(path, \"w\", encoding=\"utf-8\")\n",
        "\n",
        "    try:\n",
        "        for url in tqdm(urls):\n",
        "\n",
        "            rec = {\"url\": url, \"fetched_at\": datetime.now(timezone.utc).isoformat()}\n",
        "            try:\n",
        "                resp = server.get(url)\n",
        "                text = resp.text or \"\"\n",
        "                if \"Academic Year\" not in text:\n",
        "                    rec.update({\n",
        "                        \"ok\": False,\n",
        "                        \"status_code\": getattr(resp, \"status_code\", None),\n",
        "                        \"error\": \"Missing 'Academic Year' marker\", # Assume valid page should have this\n",
        "                        \"preview\": text[:500]\n",
        "                    })\n",
        "                else:\n",
        "                    rec.update({\n",
        "                        \"ok\": True,\n",
        "                        \"status_code\": getattr(resp, \"status_code\", None),\n",
        "                        \"html\": text\n",
        "                    })\n",
        "            except requests.exceptions.RequestException as e:\n",
        "                rec.update({\n",
        "                    \"ok\": False,\n",
        "                    \"status_code\": None,\n",
        "                    \"error\": f\"RequestException: {e}\"\n",
        "                })\n",
        "\n",
        "            f.write(json.dumps(rec, ensure_ascii=False) + \"\\n\")\n",
        "            f.flush()\n",
        "\n",
        "            time.sleep(2)\n",
        "    finally:\n",
        "        f.close()\n",
        "\n",
        "    return path\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "with open('data/raw/course_codes.txt', 'r') as f:\n",
        "    codes = f.read().splitlines()\n",
        "\n",
        "urls = [f'https://coursefeedback.uchicago.edu/?Department={code}&AcademicYear=2024&AcademicTerm=All' for code in codes]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2/2 [00:23<00:00, 11.71s/it]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'data/course_urls.jsonl'"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "get_pages_by_subject(server, urls[0:2])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from bs4 import BeautifulSoup\n",
        "\n",
        "with open('raw_data/pages_by_subject.jsonl', 'r', encoding='utf-8') as f:\n",
        "    lines = f.readlines()\n",
        "\n",
        "    links = []\n",
        "\n",
        "    for line in lines:\n",
        "        l = json.loads(line)\n",
        "        if l.get(\"ok\"):\n",
        "            soup = BeautifulSoup(l.get(\"html\", \"\"), \"html.parser\")\n",
        "            for td in soup.find_all(\"td\", class_=\"course\"):\n",
        "                a = td.find(\"a\", href=True)\n",
        "                if a:\n",
        "                    links.append(a[\"href\"])\n",
        "\n",
        "with open('data/raw/course_urls.txt', 'w', encoding='utf-8') as f:\n",
        "    for link in links:\n",
        "        f.write(link + '\\n')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gkSTe9fFgQgb"
      },
      "source": [
        "## Part 2 Obtaining Course Eval HTMLs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n6DxVlbU3xqs"
      },
      "outputs": [],
      "source": [
        "import io\n",
        "import pandas as pd\n",
        "cookies=pd.read_csv(io.StringIO(COOKIES), header=None)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "iJyxpdce8MyK"
      },
      "outputs": [],
      "source": [
        "cookies = cookies.set_index(0)[1].to_dict()\n",
        "s = requests.Session()\n",
        "s.cookies.update(cookies)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os, json, requests\n",
        "from datetime import datetime\n",
        "from tqdm import tqdm\n",
        "\n",
        "def scrape_responses_jsonl(urls, session, out):\n",
        "\n",
        "    successes = failures = 0\n",
        "\n",
        "    with open(out, \"a\", encoding=\"utf-8\") as out_f:\n",
        "        for url in tqdm(urls):\n",
        "            ts = datetime.utcnow().isoformat(timespec=\"seconds\") + \"Z\"\n",
        "            try:\n",
        "                r = session.get(url, timeout=30)\n",
        "                if \"Number Enrolled\" in r.text:\n",
        "                    rec = {\n",
        "                        \"timestamp\": ts,\n",
        "                        \"url\": url,\n",
        "                        \"status_code\": r.status_code,\n",
        "                        \"ok\": True,\n",
        "                        \"text\": r.text\n",
        "                    }\n",
        "                    successes += 1\n",
        "                else:\n",
        "                    rec = {\n",
        "                        \"timestamp\": ts,\n",
        "                        \"url\": url,\n",
        "                        \"status_code\": r.status_code,\n",
        "                        \"ok\": False,\n",
        "                        \"error\": '\"Number Enrolled\" not found'\n",
        "                    }\n",
        "                    failures += 1\n",
        "\n",
        "            except requests.exceptions.RequestException as e:\n",
        "                rec = {\n",
        "                    \"timestamp\": ts,\n",
        "                    \"url\": url,\n",
        "                    \"status_code\": None,\n",
        "                    \"ok\": False,\n",
        "                    \"error\": f\"{type(e).__name__}: {e}\"\n",
        "                }\n",
        "                failures += 1\n",
        "\n",
        "            out_f.write(json.dumps(rec, ensure_ascii=False) + \"\\n\")\n",
        "            out_f.flush()\n",
        "\n",
        "            time.sleep(0.3)\n",
        "\n",
        "    return {\"out_path\": out, \"successes\": successes, \"failures\": failures}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "with open('data/raw/course_urls.txt', 'r', encoding='utf-8') as f:\n",
        "    course_urls = f.read().splitlines()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 17/17 [00:06<00:00,  2.57it/s]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'out_path': 'raw_data/course_pages.jsonl', 'successes': 17, 'failures': 0}"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "scrape_responses_jsonl(course_urls, s, out=\"data/raw/course_pages.jsonl\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Part 3. Extracting Images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import base64\n",
        "import hashlib\n",
        "import mimetypes\n",
        "from datetime import datetime\n",
        "from typing import List, Dict, Any\n",
        "\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from urllib.parse import urljoin\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "def fetch_course_image_urls(course_url, course_html):\n",
        "    \"\"\"\n",
        "    Parse a course page's HTML and return a list of:\n",
        "      { \"question\": <str>, \"src\": <absolute-url-str> }\n",
        "    Skips blocks without an <img>.\n",
        "    \"\"\"\n",
        "    soup = BeautifulSoup(course_html, \"html.parser\")\n",
        "    out = []\n",
        "\n",
        "    for block in soup.select(\".FrequencyBlockRow\"):\n",
        "        q_el = block.select_one(\".FrequencyQuestionTitle span\")\n",
        "        question = q_el.get_text(strip=True) if q_el else \"No Question Text\"\n",
        "        img = block.select_one(\"img\")\n",
        "        src = img[\"src\"] if img and \"src\" in img.attrs else None\n",
        "        if not src:\n",
        "            continue\n",
        "\n",
        "        abs_src = urljoin(course_url or \"\", src)\n",
        "        out.append({\"question\": question, \"src\": abs_src})\n",
        "\n",
        "    return out\n",
        "\n",
        "\n",
        "def build_image_bytes_jsonl(\n",
        "    session,\n",
        "    course_pages_path = \"raw_data/course_pages.jsonl\",\n",
        "    out_path = \"raw_data/image_bytes.jsonl\",\n",
        "):\n",
        "    \"\"\"\n",
        "    Reads course_pages.jsonl lines of the form:\n",
        "      {\"url\": \"<page_url>\", \"ok\": true, \"text\": \"<html>...\"}\n",
        "    For each page, extracts images and writes one JSON object per line to `out_path`:\n",
        "      {\n",
        "        \"url\": \"<page_url>\",\n",
        "        \"images\": [\n",
        "          {\n",
        "            \"question\": \"...\",\n",
        "            \"mime\": \"image/png\",\n",
        "            \"sha256\": \"<hex>\",\n",
        "            \"n_bytes\": <int>,\n",
        "            \"data_base64\": \"<...>\"\n",
        "          },\n",
        "          # or, on error:\n",
        "          {\n",
        "            \"question\": \"...\",\n",
        "            \"src\": \"https://...\",\n",
        "            \"error\": \"RequestException: ...\"\n",
        "          }\n",
        "        ]\n",
        "      }\n",
        "    \"\"\"\n",
        "    os.makedirs(os.path.dirname(out_path), exist_ok=True)\n",
        "    pages = 0\n",
        "\n",
        "    with open(course_pages_path, \"r\", encoding=\"utf-8\") as src, \\\n",
        "         open(out_path, \"w\", encoding=\"utf-8\") as dst:\n",
        "\n",
        "        for line in tqdm(src, desc=\"Processing pages\"):\n",
        "            if not line.strip():\n",
        "                continue\n",
        "\n",
        "            try:\n",
        "                rec = json.loads(line)\n",
        "            except json.JSONDecodeError:\n",
        "                continue\n",
        "\n",
        "            if not (rec.get(\"ok\") and rec.get(\"text\") and rec.get(\"url\")):\n",
        "                continue\n",
        "\n",
        "            page_url = rec[\"url\"]\n",
        "            html = rec[\"text\"]\n",
        "\n",
        "            items = fetch_course_image_urls(page_url, html)\n",
        "\n",
        "            out_images: List[Dict[str, Any]] = []\n",
        "            seen_src = set() \n",
        "\n",
        "            for it in items:\n",
        "                q = it.get(\"question\")\n",
        "                if \"Now that\" not in q and \"Prior to\" not in q:\n",
        "                    continue\n",
        "                \n",
        "                src_url = it.get(\"src\")\n",
        "                if not src_url or src_url in seen_src:\n",
        "                    continue\n",
        "                seen_src.add(src_url)\n",
        "\n",
        "                try:\n",
        "                    r = session.get(src_url, timeout=30)\n",
        "                    r.raise_for_status()\n",
        "                    data = r.content\n",
        "\n",
        "\n",
        "                    b64 = base64.b64encode(data).decode(\"ascii\")\n",
        "                    mime = (r.headers.get(\"Content-Type\")\n",
        "                            or mimetypes.guess_type(src_url)[0]\n",
        "                            or \"application/octet-stream\")\n",
        "                    mime = mime.split(\";\")[0]\n",
        "                    sha = hashlib.sha256(data).hexdigest()\n",
        "\n",
        "                    out_images.append({\n",
        "                        \"question\": q,\n",
        "                        \"mime\": mime,\n",
        "                        \"sha256\": sha,\n",
        "                        \"n_bytes\": len(data),\n",
        "                        \"data_base64\": b64\n",
        "                    })\n",
        "\n",
        "                except requests.RequestException as e:\n",
        "                    out_images.append({\n",
        "                        \"question\": q,\n",
        "                        \"src\": src_url,\n",
        "                        \"error\": f\"{type(e).__name__}: {e}\"\n",
        "                    })\n",
        "\n",
        "            dst.write(json.dumps({\"url\": page_url, \"images\": out_images}, ensure_ascii=False) + \"\\n\")\n",
        "            pages += 1\n",
        "\n",
        "    return {\"out_path\": out_path, \"pages_processed\": pages}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing pages: 17it [00:00, 23.94it/s]\n"
          ]
        }
      ],
      "source": [
        "result = build_image_bytes_jsonl(\n",
        "        session=s,\n",
        "        course_pages_path=\"data/raw/course_pages.jsonl\",\n",
        "        out_path=\"data/raw/image_bytes.jsonl\"\n",
        "    )"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
